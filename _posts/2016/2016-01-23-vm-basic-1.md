---
layout: post
title: VM Basic - 1
description: VMware存储栈基本概念介绍。关键字：虚拟化，虚拟机，LUN, Volume, VMFS，Datastore, 虚拟机文件，虚拟磁盘，PVSCSI，vSAN。
categories:
- [Chinese, Software]
tags:
- [kernel, file system, virtual machine, virtualization, storage, cloud]
---

## VMware存储栈那点儿事儿

解释什么是[Virtual Machine](https://en.wikipedia.org/wiki/Virtual_machine)的工作就交给万能的维基百科了。

本篇文章只限于VMware vShpere VMKernel的Storage Stack(存储栈)的介绍。这里要说的一些概念
都是VMware虚拟化技术里存储相关的概念。VMware vShere的存储栈是VMKernel的重要组成部分，存储栈的大图可以从
[VMKernel Storage TOI](https://github.com/yangoliver/mydoc/raw/19fc6e7f89002f6ada7f7141979b273f8a369510/share/vmkernel_storage_toi.pdf)
下载到。我们就按照存储栈自下而上的顺序来一一介绍。

###1. Storage Device Drivers

和其它操作系统一样，VMKernel的存储栈最底层就是驱动程序和驱动程序的框架了。而驱动和硬件的生态系统通常都是非Windows和Linux操作系统的一大弱点。
VMKernel对硬件驱动程序的支持分为两大类，

- **Linux Device Drivers** and **Linux Emulation Layer**

  早期的vSphere VMKernel用了一个巧妙的办法来解决这个问题，那就是通过Linux驱动模拟层(vmklinux)来让Linux驱动源代码可以和VMKernel链接在一
  起。这个Linux模拟层为Linux原生驱动程序提供和Linux一致的API，降低驱动移植的代价。这种方式利用了Linux良好的硬件驱动生态系统，为vSphere的硬件
  兼容支持铺平了道路。

  个人理解，Linux模拟层的另一个作用就把GPL许可的Linux驱动和商业许可的VMKernel隔离起来，解决中间的法律风险。(本人是法盲，但Linux兼容层的设计和代码绝对
  是经过律师专业的review，经得起考验的。)

  Linux模拟层的代码在存储栈里对应了IDE，SCSI，Block，iSCSI，FC，FCOE驱动的所有相关协议API的模拟，应该还是有一定复杂度的。

- **Native Device Drivers** and **VMKernel Device Layer**

  Linux驱动兼容层的做法虽然巧妙，但带来了性能，可靠性和稳定性的一些问题。vSphere 5.5开始，Native Device Driver Architecture(原生设备驱动架构)被引入，
  VMKernel开始支持**VMKernel Device Layer**，并提供VMK API Device Driver Development Kit来支持原生驱动的开发。因为有了原生驱动，驱动的性能和功能都有
  所增强，比如一致性的debug体验和PCIe热插拔支持。

目前的VMKernel的驱动是所谓的hybrid(混血)方案，即Linux驱动和VMKernel原生驱动同时存在。原生驱动是否能一统内核不但需要VMware去推广，更需要硬件厂商积极
支持。这是个长期的过程，所以相信在相当长的时间里，VMKernel的驱动会保持这个状态。

###2. Adapter I/O schedulers

很难查到VMKernel Adapter I/O schedulers的介绍。这一层位于存储驱动之上，而且根据名字，个人猜想这个IO调度器更多的是完成和Linux驱动之上IO调度器相似的功
能，主要可能是，

- 对上层发往物理存储的IO请求排队，合并IO，优化对存储(如HDD)的访问。
- 控制来自上层IO请求的优先级。
- 物理IO请求统计功能。

###3. Storage APIs

这一层提供VMKernel存储栈的API，给第三方的存储硬件和软件相关功能提供一个framework。

目前vSphere提供了下面这些API，

- **VAAI(vStorage APIs for Array Integration)**

  1. Hardware Acceleration APIs

     允许把一些VM存储工作负载交给存储阵列hardware来处理。
	 
  2. Array Thin Provisioning APIs

     帮助监视使用thin provision的阵列上的空间使用情况，防止空间用尽，并执行空间回收。

- **VAMP(vStorage APIs for Multipathing)**

  提供了PSA(Pluggable Storage Architecture)，支持第三方存储模块插件。

  在这一层VMware有自己的NMP(Native Multipathing Plug-In)实现。第三方存储厂商可以独立于vSphere产品外利用PSA来开发为自己阵列优化的multi-path插件，实现自
  己的failover，load balance和性能改进。

- **VASA(vStorage APIs for Storage Awareness)**

  提供给阵列厂商API来通知vCenter server来拿到存储的配置，硬件能力(capability)，健康状态，存储管理事件。

- **VADP(vStorage APIs for Data Protection)**

  VADP是为backup和Data proection软件厂商提供了一种不需要在VM内部安装备份Agent软件就可以实现VM备份的机制。这种机制消除了传统备份Agent在VM内部的工作负
  载，减少了因此而带来的额外开销。VADP利用了vSphere snapshot机制，使得VM备份到SAN存储上而不需要停机。因此，VM备份可以做到应用工作时间不中断，也不需要
  延长应用和业务的备份窗口，或者因为备份而暂停应用和业务。

  VADP的快照是通过CBT(Change Block Tracking)，让备份软件支持对VM的增量备份，大大的提高了VM备份效率。

###4. Logical I/O scheduler

Logical IO scheduler位于Storage API层之上，主要有以下功能，

- 将发到逻辑设备上的IO请求排队
- 针对上层多个VM发来的IO请求，进行VM之间的QOS处理，支持如下几个方面，
  1. 最小预留IO带宽的下限。
  2. 最大IO资源上限。
  3. IO带宽资源的share。
- 逻辑IO请求统计功能。

###5. Block Layer

这一层有以下模块，用于支持块设备的管理，

- **Block Driver**

  磁盘驱动。

- **LVM Driver**

  LVM作为逻辑卷管理器提供了在多块物理磁盘上创建逻辑卷的功能。 

块设备层为VMKernel的上层文件系统提供了一个逻辑上的块设备，也就是LUN。

**[LUN(Logical Unit Number)](https://en.wikipedia.org/wiki/Lun)(逻辑单元编号)**最早由SCSI协议引入，是SCSI总线协议寻址的设备地址。
后来越来越多的被引申为Logical Disk(逻辑磁盘)或者Logical Volume(逻辑卷)。

一个LUN可以由多个硬盘组成。任何外部的磁盘阵列都可以把多个物理盘划分到一个LUN里。从主机的角度看，一个LUN就是一块物理硬盘。
可以说LUN就是外置存储设备或者LVM程序对物理盘的虚拟化。

###6. File System Switch & File System Modules

很难查到FSS(File System Switch)的资料。这一层架构在具体文件系统模块之上，貌似提供了一些简单的接口封装。VMKernel支持多种文件系统模块，如NFS，
VMFS的各种版本的模块实例由FSS层转发到具体文件系统模块。诸多文件系统中，最重要的当然是VMFS。另外，Datastore的概念也和文件系统密切相关。

####6.1 VMFS(Virtual Machine File System)

VMware vSphere VMFS不但支持VMFS Volume管理，而且同时还是一种高性能的Cluster File System，并专为虚拟机优化。

- **VMFS Volume**

  在VMware的环境里，本地或者外部的存储一旦划分出了LUN，它就可以在这之上创见VMFS Volume。一个VMFS Volume通常只包含一个LUN，
  但是也可以由多个VMFS Extent组成，每个VMFS Extent都是一个LUN。

- **Cluster File System**

  [Linux File System Basic - 1](http://oliveryang.net/2016/01/linux-file-system-basic-1/)这篇文章里，涉及到Cluster FS的
  分类，而VMFS就属于Shared-disk文件系统这种架构。这就意味着，VMFS可以借助共享存储，如NAS，SAN存储，来实现多个VMware
  vSphere主机对同一文件系统的并发读写操作。而传统的本地文件系统，如Ext4，XFS，是无法允许多个主机同时mount和读写同一文件
  系统的。

  正因为VMFS是集群文件系统，才使得虚拟机可以跨越单个物理机范围去扩展。可以说VMFS是VM快照，精简配置(Thin Provision)，VM的
  热迁移(VM vMotion), DRS(Distributed Resource Scheduler), HA(High Availability)，Storage vMotion等一系列重要特性的基础。

####6.2 Datastore

Datastore是VMware存储里抽象出的一个概念，用于存储虚拟机文件(Virtual Machine Files)。VMware文档里是这么描述的，

<pre>Datastores are logical containers, analogous to file systems, that hide
specifics of each storage device and provide a uniform model for storing
virtual machine files.</pre>

说直白一点，Datastore就是VMware为虚拟机提供的存储抽象，目前VMware可以在下面几种文件系统和卷(Volume)上建立Datastore,

1. **VMFS Datastore**

   VMFS Datastore需要在LUN之上创建VMFS Volume和文件系统。

   VMFS可以用于在服务器的DAS上创建只为本地虚拟机服务的文件系统，也可以用于在传统共享存储之上，如SAN(FC，iSCSI)，来创建允许
   多个vSphere主机上的不同虚拟机共享的集群文件系统。

2. **NFS Datastore**

   直接使用第三方的NFS文件系统服务或者存储设备。

   在虚拟化时代，块接口访问方式大行其道，NFS Datastore显然不如其它方式更受用户的欢迎。

3. **VSAN(VMware Virtual SAN) Datastore**

   在VSAN的分布式对象文件系统上建立Datastore。

   VMware全新的Cluster File System，和VMFS不同，它不需要在共享存储上建立文件系统集群。而是真正的基于DAS来建立的分布式文件
   系统。关于VSAN的信息，可以查看[VSAN use case summary](http://oliveryang.net/2016/01/vsan-use-case-summary/)这篇文章。

4. **VVol(VMware Virtual Volume) Datastore**

   在第三方外置存储提供的VVol卷上创建Datastore。

   VVol是为第三方外置存储提供的软件定义解决方案。VM可以在VVol之上创建自己的Virtual Datastore。VVol和VSAN都是VMware推出的
   软件定义存储解决方案。只不过针对不同的用户需求和市场。VSAN作为HCI架构的VMware原生存储方案，将会是未来发展的主方向。

大多数情况下，VMware建议一个Volume里只包含一个LUN。这时，Datastore和Volume是对等的关系。

###7. Virtual Machine Files

虚拟机就是由内存里的数据和存在Datastore里VM home目录下的一组文件来共同组成的。在VMware vSphere里，共有11种不同扩展名的虚
拟机文件。其中最关键的要数下面几种，

- *.vmx 文件，需用存储虚拟机的配置。
- *.vmdk文件，虚拟磁盘文件，在虚拟机中被Guest OS识别为块设备。
- *.nvram文件，存放虚拟机BIOS和EFI固件的配置信息。
- *.log文件，虚拟机的关键活动日志，用于debug。
- *.vswp文件，存放虚拟机的虚拟内存交换数据。
- *.vmsd和*.vmsn，存放虚拟机的快照元数据和数据。

**Virtual Disks**用来在VM里模拟物理磁盘。在vSphere里，虚拟磁盘很多是利用vmdk文件模拟的。vSphere目前支持以下虚拟磁盘格式，

1. **Thick disks**

   磁盘在Datastore里的空间在创建时就被分配好了。

   - Lazy Zeroed Thick，数据在使用时清零，创建时不清零。

   - Eager Zeroed Thick，数据在创建时即清零。

2. **Thin disks**

   支持精简配置。磁盘创建时实际的容量很小，随着使用动态增加存储。

3. **RDM disks**

   Virtual compatibility mode raw disk mapping，它使得虚拟机可以直接访问第三方存储提供的LUN。RDM实际上在VMFS上创建了一个特
   殊文件- RDM，并把存储系统(如SAN)的裸设备(LUN)映射到这个文件上。RDM就是为VM直接使用外部第三方存储设计的。

4. **RDMP disks**

   Physical compatibility mode (pass-through) raw disk mapping。即把一个pass-through的裸设备直接映射到VMFS上的一个文件。和
   RDM的区别是，这时VM访问这个虚拟磁盘不需要经过ESXi的SCSI命令模拟层处理了。

5. **2GB sparse disks**

   是一种特殊的磁盘格式，2GB大小。该磁盘不能用于运行VM，而是用于虚拟机导入和导出用途。要想运行VM，必须把这个格式的磁盘文件
   用工具转换成Thick或者Zero disks。

###8. SCSI Command Emulation

VMKernel的SCSI命令模拟层让VM Guest OS里的驱动能够正常工作，像操作物理磁盘一样操作虚拟磁盘来完成存储功能。

SCSI命令模拟层有以下两种类型，

- **Virtual SCSI Protocol(VSCSI)**

  Para-virtualization技术。
  通过支持VSCSI协议，VM Guest OS的PV驱动就可以很高效的完成磁盘IO操作。VMware vSphere在Guest OS里的PV驱动是PVSCSI驱动。

- **SCSI Hardware Emulation**

  Full Virtualization技术。
  VMware vSphere可以模拟多种存储HBA硬件，这时VM Guest OS里加载的是操作系统原生的驱动程序。例如，模拟LSI的SAS HBA卡，Linux Guest里运行mptsas驱动。

## 小结

本文中的内容来自
[vSphere Storage Guide](http://pubs.vmware.com/vsphere-60/topic/com.VMware.vsphere.storage.doc/GUID-8AE88758-20C1-4873-99C7-181EF9ACFA70.html)
和互联网搜索。由于资料有限，部分属于个人猜测的信息已经在文章中做了明确的提示。作者会不断更新文档以修正任何错误疏漏。
